[package]
name = "stt"
version = "0.1.0"
edition = "2021"
description = "Fast speech-to-text inference in Rust. CoreML (ANE), DirectML, CUDA."
license = "MIT"
repository = "https://github.com/screenpipe/stt"
keywords = ["speech-to-text", "asr", "whisper", "parakeet", "onnx"]
categories = ["multimedia::audio", "science"]

[dependencies]
ort = { version = "2.0.0-rc.11", default-features = false, features = ["std", "ndarray", "half"], optional = true }
hf-hub = "0.4"
ndarray = { version = "0.17", optional = true }
realfft = { version = "3", optional = true }
thiserror = "2"
tracing = "0.1"
serde = { version = "1", features = ["derive"] }
serde_json = "1"
half = { version = "2", optional = true }
whisper-rs = { version = "0.15", optional = true }
qwen3-asr-sys = { path = "qwen3-asr-sys", optional = true }

[dev-dependencies]
hound = "3"
tracing-subscriber = "0.3"

[[example]]
name = "transcribe"
path = "examples/transcribe.rs"

[[example]]
name = "benchmark"
path = "examples/benchmark.rs"

[[example]]
name = "bench_batch"
path = "examples/bench_batch.rs"

[[example]]
name = "test_ggml"
path = "examples/test_ggml.rs"

[features]
default = ["ort-defaults"]

# ONNX Runtime deps (needed for parakeet, qwen3-asr ONNX engines)
onnx-deps = ["dep:ort", "dep:ndarray", "dep:realfft", "dep:half"]
ort-defaults = ["onnx-deps", "ort/default"]

# Execution providers â€” pick one (or none for CPU)
cuda = ["onnx-deps", "ort/cuda"]
tensorrt = ["onnx-deps", "ort/tensorrt"]
coreml = ["onnx-deps", "ort/coreml"]
directml = ["onnx-deps", "ort/directml"]

# Models
whisper = ["dep:whisper-rs"]
parakeet = ["onnx-deps"]
qwen3-asr = ["onnx-deps"]
qwen3-asr-ggml = ["dep:qwen3-asr-sys"]
all-models = ["whisper", "parakeet", "qwen3-asr"]

# GGML GPU backends
metal = ["qwen3-asr-ggml", "qwen3-asr-sys/metal"]
vulkan-ggml = ["qwen3-asr-ggml", "qwen3-asr-sys/vulkan"]
cuda-ggml = ["qwen3-asr-ggml", "qwen3-asr-sys/cuda"]
